{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Prefer repartition to coalesce in Spark\n",
    "authors:\n",
    "- Ryan Harter (:harter) \n",
    "tags:\n",
    "- Spark\n",
    "- ATMO\n",
    "created_at: 2017-03-02\n",
    "updated_at: 2017-03-02\n",
    "tldr: When saving data to parquet in Spark/ATMO, avoid using coalesce. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I ran into some Spark weirdness when working on some ETL.\n",
    "Specifically, when repartitioning a parquet file with `coalesce()`, the parallelism for the entire job (including upstream tasks) was constrained by the number of coalesce partitions.\n",
    "Instead, I expected the upstream jobs to use all available cores.\n",
    "We should be limited by the number of file partitions only when its time to actually write the file.\n",
    "\n",
    "It's probably easier if I demonstrate.\n",
    "Below I'll create a small example dataframe containing 10 rows.\n",
    "I'll map a slow function over the example dataframe in a few different ways.\n",
    "I'd expect these calculations to take a fixed amount of time, since they're happening in parallel.\n",
    "However, for one example, **execution time will increase linearly with the number of rows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "path = \"~/tmp.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slow_func(ping):\n",
    "    \"\"\"Identity function that takes 1s to return\"\"\"\n",
    "    time.sleep(1)\n",
    "    return(ping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"Times the execution of a function\"\"\"\n",
    "    start_time = time.time()\n",
    "    func()\n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "timer(lambda: slow_func(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_frame(rdd):\n",
    "    return sqlContext.createDataFrame(rdd, schema=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RDD\n",
    "\n",
    "First, let's look at a simple RDD. Everything seems to work as expected here. Execution time levels off to ~3.7 as the dataset increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda x: timer(lambda: sc.parallelize(range(x)).map(slow_func).take(x)), range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "Let's create a Spark DataFrame and write the contents to parquet without any modification. Again, things seem to be behaving here. Execution time is fairly flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda x: timer(lambda: create_frame(sc.parallelize(range(x)))\\\n",
    "                                .coalesce(1).write.mode(\"overwrite\").parquet(path)),\n",
    "    range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offending Example\n",
    "\n",
    "Now, let's map the slow function over the DataFrame before saving. This should increase execution time by one second for every dataset. However, it looks like **execution time is increasing by one second for each row**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(lambda x: timer(lambda: create_frame(sc.parallelize(range(x))\\\n",
    "                                .map(slow_func))\\\n",
    "                                .coalesce(1).write.mode(\"overwrite\").parquet(path)),\n",
    "    range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartition fixes the issue\n",
    "\n",
    "Using `repartition` instead of `coalesce` fixes the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(lambda x: timer(lambda: create_frame(sc.parallelize(range(x))\\\n",
    "                                .map(slow_func))\\\n",
    "                                .repartition(1).write.mode(\"overwrite\").parquet(path)),\n",
    "    range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}