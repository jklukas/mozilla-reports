{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Containers Testpilot Pipeline\n",
    "authors:\n",
    "- Ryan Harter (:harter) \n",
    "tags:\n",
    "- Spark\n",
    "- ATMO\n",
    "- ETL\n",
    "created_at: 2017-03-08\n",
    "updated_at: 2017-03-08\n",
    "tldr: Populates containers_testpilottest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ~/cliqz_ping_pipeline/transform.py\n",
    "import ujson as json\n",
    "from datetime import *\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split\n",
    "import base64\n",
    "from Crypto.Cipher import AES\n",
    "\n",
    "from moztelemetry import get_pings_properties\n",
    "from moztelemetry.dataset import Dataset\n",
    "\n",
    "class ColumnConfig:\n",
    "    def __init__(self, name, path, cleaning_func, struct_type):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.cleaning_func = cleaning_func\n",
    "        self.struct_type = struct_type\n",
    "\n",
    "class DataFrameConfig:\n",
    "    def __init__(self, col_configs, ping_filter):\n",
    "        self.columns = [ColumnConfig(*col) for col in col_configs]\n",
    "        self.ping_filter = ping_filter\n",
    "\n",
    "    def toStructType(self):\n",
    "        return StructType(map(\n",
    "            lambda col: StructField(col.name, col.struct_type, True),\n",
    "            self.columns))\n",
    "\n",
    "    def get_names(self):\n",
    "        return map(lambda col: col.name, self.columns)\n",
    "\n",
    "    def get_paths(self):\n",
    "        return map(lambda col: col.path, self.columns)\n",
    "\n",
    "\n",
    "def pings_to_df(sqlContext, pings, data_frame_config):\n",
    "    \"\"\"Performs simple data pipelining on raw pings\n",
    "\n",
    "    Arguments:\n",
    "        data_frame_config: a list of tuples of the form:\n",
    "                 (name, path, cleaning_func, column_type)\n",
    "    \"\"\"\n",
    "    filtered_pings = get_pings_properties(pings, data_frame_config.get_paths())\\\n",
    "        .filter(data_frame_config.ping_filter)\n",
    "\n",
    "    return config_to_df(sqlContext, filtered_pings, data_frame_config)\n",
    "\n",
    "def config_to_df(sqlContext, raw_data, data_frame_config):\n",
    "    \"\"\"Performs simple data pipelining on raw pings\n",
    "\n",
    "    Arguments:\n",
    "        data_frame_config: a list of tuples of the form:\n",
    "                 (name, path, cleaning_func, column_type)\n",
    "    \"\"\"\n",
    "    def build_cell(ping, column_config):\n",
    "        \"\"\"Takes a json ping and a column config and returns a cleaned cell\"\"\"\n",
    "        raw_value = ping[column_config.path]\n",
    "        func = column_config.cleaning_func\n",
    "        if func is not None:\n",
    "            return func(raw_value)\n",
    "        else:\n",
    "            return raw_value\n",
    "\n",
    "    def ping_to_row(ping):\n",
    "        return [build_cell(ping, col) for col in data_frame_config.columns]\n",
    "\n",
    "    return sqlContext.createDataFrame(\n",
    "        raw_data.map(ping_to_row).collect(),\n",
    "        schema = data_frame_config.toStructType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df(df, name, date_partition, partitions=1):\n",
    "    if date_partition is not None:\n",
    "        partition_str = \"/submission_date={day}\".format(day=date_partition)\n",
    "    else:\n",
    "        partition_str=\"\"\n",
    "\n",
    "    # TODO: this name should include the experiment name\n",
    "    path_fmt = \"s3n://telemetry-parquet/harter/containers_{name}/v1{partition_str}\"\n",
    "    path = path_fmt.format(name=name, partition_str=partition_str)\n",
    "    df.repartition(partitions).write.mode(\"overwrite\").parquet(path)\n",
    "\n",
    "def __main__(sc, sqlContext, day=None, save=True):\n",
    "    if day is None:\n",
    "        # Set day to yesterday\n",
    "        day = (date.today() - timedelta(1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    get_doctype_pings = lambda docType: Dataset.from_source(\"telemetry\") \\\n",
    "        .where(docType=docType) \\\n",
    "        .where(submissionDate=day) \\\n",
    "        .where(appName=\"Firefox\") \\\n",
    "        .records(sc)\n",
    "\n",
    "    testpilottest_df = pings_to_df(\n",
    "        sqlContext,\n",
    "        get_doctype_pings(\"testpilottest\"),\n",
    "        DataFrameConfig(\n",
    "            [\n",
    "                (\"uuid\", \"payload/payload/uuid\", None, StringType()),\n",
    "                (\"userContextId\", \"payload/payload/userContextId\", None, LongType()),\n",
    "                (\"clickedContainerTabCount\", \"payload/payload/clickedContainerTabCount\", None, LongType()),\n",
    "                (\"eventSource\", \"payload/payload/eventSource\", None, StringType()),\n",
    "                (\"event\", \"payload/payload/event\", None, StringType()),\n",
    "                (\"hiddenContainersCount\", \"payload/payload/hiddenContainersCount\", None, LongType()),\n",
    "                (\"shownContainersCount\", \"payload/payload/shownContainersCount\", None, LongType()),\n",
    "                (\"totalContainersCount\", \"payload/payload/totalContainersCount\", None, LongType()),\n",
    "                (\"totalContainerTabsCount\", \"payload/payload/totalContainerTabsCount\", None, LongType()),\n",
    "                (\"totalNonContainerTabsCount\", \"payload/payload/totalNonContainerTabsCount\", None, LongType()),\n",
    "                (\"test\", \"payload/test\", None, StringType()),\n",
    "            ],\n",
    "            lambda ping: ping['payload/test'] == \"@testpilot-containers\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        save_df(testpilottest_df, \"testpilottest\", day, partitions=1)\n",
    "\n",
    "    return testpilottest_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpt = __main__(sc, sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpt.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}