{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Telemetry Hello World\"\n",
    "authors:\n",
    "- vitillo\n",
    "tags:\n",
    "- tutorial\n",
    "- examples\n",
    "- telemetry\n",
    "- spark\n",
    "created_at: 2016-03-10\n",
    "updated_at: 2018-05-25\n",
    "tldr: Brief introduction to Spark and Telemetry in Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telemetry Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very a brief introduction to Spark and Telemetry in Python. You should have a look at the [tutorial](https://gist.github.com/vitillo/25a20b7c8685c0c82422) in Scala and the associated [talk](http://www.slideshare.net/RobertoAgostinoVitil/spark-meets-telemetry) if you are interested to learn more about Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from moztelemetry.dataset import Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Dataset API to fetch data.  Documentation can be found at: https://python-moztelemetry.readthedocs.io/en/stable/api.html#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this example is to plot the startup distribution for each OS. Let's see how many parallel workers we have at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the schema of the dataset we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_source('telemetry').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Dataset of Telemetry submissions for a given submission date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pings_dataset = (\n",
    "    Dataset.from_source('telemetry')\n",
    "    .where(docType='main')\n",
    "    .where(submissionDate='20180105')\n",
    "    .where(appUpdateChannel=\"nightly\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the properties we need and then take a 10% sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pings = (\n",
    "    pings_dataset\n",
    "    .select(\n",
    "        'clientId',\n",
    "        osName='environment.system.os.name',\n",
    "        firstPaint='payload.simpleMeasurements.firstPaint')\n",
    "    .records(sc, sample=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out submissions with an invalid startup time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pings.filter(lambda p: p.get('firstPaint', -1) >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent pseudoreplication, let's consider only a single submission for each client. As this step requires a distributed shuffle, it should always be run only after extracting the attributes of interest with *Dataset.select()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = (\n",
    "    subset\n",
    "    .map(lambda p: (p['clientId'], p))\n",
    "    .reduceByKey(lambda p1, p2: p1)\n",
    "    .map(lambda p: p[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is fundamental as it allows for an iterative, real-time development workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = subset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many pings are we looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group the startup timings by OS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    cached\n",
    "    .map(lambda p: (p['osName'], p['firstPaint']))\n",
    "    .groupByKey()\n",
    "    .collectAsMap()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({x: np.log10(pd.Series(list(y))) for x, y in grouped.items()})\n",
    "plt.figure(figsize=(17, 7))\n",
    "frame.boxplot(return_type='axes')\n",
    "plt.ylabel('log10(firstPaint)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('startup distribution for Windows')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('log10(firstPaint)')\n",
    "frame['Windows_NT'].plot(kind='hist', bins=50, figsize=(14, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract a histogram of GC_MARK_MS (time spent running JS garbage collection mark phase) from the submissions:\n",
    "\n",
    "(see https://developer.mozilla.org/en-US/docs/Web/JavaScript/Memory_Management for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histograms = (\n",
    "    pings_dataset\n",
    "    .select(GC_MARK_MS_content='payload.processes.content.histograms.GC_MARK_MS.values',\n",
    "            GC_MARK_MS_parent='payload.histograms.GC_MARK_MS.values')\n",
    "    .records(sc, sample=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `payload.histograms.GC_MARK_MS.values` is a path to the GC_MARK_MS values of the parent (main) process\n",
    "- `payload.processes.content.histograms.GC_MARK_MS.values` is a path to the GC_MARK_MS values of the child processes\n",
    "\n",
    "Let's aggregate the histogram over all submissions and plot it as a histogram.  Since the parent and child processes are recorded separately, we can create a histogram for each one and then add them together.\n",
    "\n",
    "Each histogram is a pandas series where the index is the bucket and the value is the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def aggregate_series(s1, s2):\n",
    "    \"\"\"Function to sum up series; if one is None, return other\"\"\"\n",
    "    if s1 is None:\n",
    "        return s2\n",
    "    if s2 is None:\n",
    "        return s1\n",
    "    return s1.add(s2, fill_value=0)\n",
    "\n",
    "aggregated_content = (\n",
    "    histograms\n",
    "    .map(lambda p: pd.Series(p['GC_MARK_MS_content']))\n",
    "    .reduce(aggregate_series)\n",
    ")\n",
    "aggregated_content.index = [int(i) for i in aggregated_content.index]\n",
    "aggregated_content = aggregated_content.sort_index()\n",
    "\n",
    "aggregated_parent = (\n",
    "    histograms\n",
    "    .map(lambda p: pd.Series(p['GC_MARK_MS_parent']))\n",
    "    .reduce(aggregate_series)\n",
    ")\n",
    "aggregated_parent.index = [int(i) for i in aggregated_parent.index]\n",
    "aggregated_parent = aggregated_parent.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('GC_MARK_MS_content')\n",
    "aggregated_content.plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('GC_MARK_MS_parent')\n",
    "aggregated_parent.plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also aggregate the values of the parent and children processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('GC_MARK_MS')\n",
    "(aggregated_content + aggregated_parent).plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyed histograms follow a similar pattern. To extract a keyed histogram for which we know the key/label we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keyed_hist = (\n",
    "    pings_dataset\n",
    "    .select(redirects='payload.keyedHistograms.NETWORK_HTTP_REDIRECT_TO_SCHEME.https.values')\n",
    "    .records(sc, sample=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add up the counts of every ping and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregated = (\n",
    "    keyed_hist\n",
    "    .filter(lambda p: p['redirects'] is not None)\n",
    "    .map(lambda p: pd.Series(p['redirects']))\n",
    "    .reduce(lambda c1, c2: c1 + c2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregated.plot(kind='bar', figsize=(15, 7))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
